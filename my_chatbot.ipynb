{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt46qTHGBaQCZT1n3hwJ3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minahil-official/chat_bot_prac/blob/main/my_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "wdw27VfY68ri",
        "outputId": "ab0bf5b1-2bd4-4c17-b46f-d86e728d1189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyDCnLlWna1UxFko_0l_vN3tDXOfeOa1Nl0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY_1')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google.generativeai"
      ],
      "metadata": {
        "id": "8gU8M4U9DWAa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "6ksA-hXFExFM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"AIzaSyDCnLlWna1UxFko_0l_vN3tDXOfeOa1Nl0\")"
      ],
      "metadata": {
        "id": "yXdRyhubFuHu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gemini-1.5-flash\""
      ],
      "metadata": {
        "id": "Fbgq9hevHT9u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name)"
      ],
      "metadata": {
        "id": "vJfWqzVlHRx-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"terms/Parameters to Explain:\n",
        "Messages\n",
        "Model\n",
        "Max Completion Tokens\n",
        "n\n",
        "Stream\n",
        "Temperature\n",
        "Top_p\n",
        "Tools\"\"\")"
      ],
      "metadata": {
        "id": "BEMY3jCzINXR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEyAAQ9VIjWB",
        "outputId": "b0d1db4e-4bc0-4d3f-e522-db19768d9415"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's break down these terms and parameters used in the context of large language models (LLMs) like GPT-3, GPT-4, and similar systems:\n",
            "\n",
            "**1. Messages:**\n",
            "\n",
            "* **Definition:**  In many modern LLM APIs, the input is structured as a series of \"messages.\"  Each message is a dictionary-like object containing a `role` (e.g., \"system,\" \"user,\" \"assistant\") and a `content` (the actual text). This allows for more complex conversational contexts than a simple single prompt.\n",
            "\n",
            "* **Example:**  A conversation might start with a system message setting the tone, followed by user messages providing instructions and assistant messages providing responses.  This structure facilitates multi-turn dialogues and maintaining context across exchanges.\n",
            "\n",
            "**2. Model:**\n",
            "\n",
            "* **Definition:** This refers to the specific large language model being used.  Different models (e.g., \"text-davinci-003,\" \"gpt-3.5-turbo,\" \"gpt-4\") have different architectures, training data, capabilities, and costs.  Choosing the right model depends on your needs in terms of performance, cost, and desired style of output.\n",
            "\n",
            "**3. Max Completion Tokens:**\n",
            "\n",
            "* **Definition:** This parameter limits the length of the model's response. Tokens are units of text (roughly words or parts of words).  Setting a lower value produces shorter responses, while a higher value allows for longer, more detailed outputs.  However, excessively high values can increase both cost and the risk of rambling or incoherent responses.\n",
            "\n",
            "**4. n (Number of Completions):**\n",
            "\n",
            "* **Definition:** This parameter specifies how many different responses you want the model to generate for a single prompt.  `n=1` gives you a single response, `n=3` gives you three different possible completions, allowing you to choose the best one or compare alternatives.  Note that requesting multiple completions increases processing time and cost.\n",
            "\n",
            "**5. Stream:**\n",
            "\n",
            "* **Definition:** This refers to whether the model's response is returned all at once or incrementally, \"streaming\" the output word by word or token by token.  Streaming is useful for interactive applications where you want to display the response as it's generated, providing a more dynamic user experience.  Non-streaming returns the complete response only after the model finishes processing.\n",
            "\n",
            "**6. Temperature:**\n",
            "\n",
            "* **Definition:** This parameter controls the randomness of the model's output.  A temperature of 0 will produce the most likely, deterministic response (often repetitive and predictable).  Higher temperatures (e.g., 0.8 or 1.0) make the output more creative, surprising, and diverse, but also risk producing less coherent or relevant results.\n",
            "\n",
            "**7. Top_p (Nucleus Sampling):**\n",
            "\n",
            "* **Definition:** An alternative to temperature, top_p controls the diversity of the output by considering only the most likely tokens whose cumulative probability exceeds the specified `top_p` value.  For example, `top_p=0.9` considers only the most likely tokens until their probabilities sum to at least 0.9. This often produces more focused and coherent outputs compared to temperature alone.  It's often preferred over temperature in many applications.\n",
            "\n",
            "**8. Tools:**\n",
            "\n",
            "* **Definition:** (Relatively new feature) This allows you to extend the capabilities of the LLM by integrating it with external tools.  These tools could be anything from code interpreters (allowing the model to execute code) to web search capabilities (letting the model access and process information from the internet).  This enables the model to perform actions beyond simple text generation.  For example, you might give the model access to a database or a calculator to enhance its problem-solving abilities.\n",
            "\n",
            "\n",
            "These parameters offer a considerable amount of control over the behavior and output of large language models, allowing you to fine-tune their responses to specific needs and applications.  The optimal settings will often depend heavily on the specific task and desired outcome.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cXddX9XJ25d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}